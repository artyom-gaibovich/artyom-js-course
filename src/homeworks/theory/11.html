<!DOCTYPE html>
<html>
	<head>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1">
		<link rel="stylesheet" type="text/css" href="css/page.css">
		<link rel="stylesheet" type="text/css" href="css/katex.css">
		<title></title>
	</head>
	<body>
		<div class="page">
			<div class="sidebar">
				<a id="nextPage">
					<img src="css/icons/forward.svg" title="Далее">
				</a>
				<a id="prevPage">
					<img src="css/icons/backward.svg" title="Назад">
				</a>
				<a href="index.html">
					<img src="css/icons/submenu.svg" title="Подменю">
				</a>
				<a href="../../../index.html">
					<img src="css/icons/menu.svg" title="Главное меню">
				</a>
				<a href="https://t.me/quessentry">
					<img src="css/icons/tg.svg" class="icon" title="Связь со мной">
				</a>
			</div>
			<div class="elements">
				<div class="topic">
					<div class="num">
						<p>11</p>
					</div>
					<div class="title">
						<p>Advantage actor-critic. Вывод.</p>
					</div>
				</div>
				<div class="content">
					<p class="redline">Основная идея A2C заключается в использовании преимущества (advantage) для обновления политики, что позволяет снизить дисперсию оценок и ускорить сходимость.</p>
					<p>Основные компоненты</p>
					<ol class="roman">
						<li>Политика $\pi_{\theta}(a \mid s)$ — функция, параметризованная $\theta$, , которая определяет вероятность выбора действия $a$ в состоянии $s$.</li>
						<li>Функция ценности состояния $V_{\phi}(s)$  — функция, параметризованная $\phi$, которая оценивает ожидаемую награду при старте из состояния $s$ и следовании политике $\pi_{\theta}$.</li>
						<li>Функция преимущества $A^{\pi}(s, a)$ — мера того, насколько лучше или хуже действие $a$ по сравнению со средним действием в состоянии $s$ при следовании политике $\pi$. Определяется как:
							<p class="equation">$A^{\pi}(s, a) = Q^{\pi}(s,a) - V^{\pi}(s)$.</p>
							<p>Преимущество — это разница между тем, насколько хорошим было действие, и тем, насколько хороша текущая ситуация в целом.</p>
							<p>Например: "ты получил награду 10, но обычно в этой ситуации получаешь 7. Значит, твое действие было на 3 лучше, чем обычно". Это помогает Актору понять, какие действия действительно полезны, а какие просто случайно оказались удачными.</p>
						</li>
						<p>Целевая функция:</p>
						<p class="equation">$J(\theta) = \mathbb{E}_{\tau \sim \pi{\theta}}\bigg[\sum^T\limits_{t=0}\gamma^t r_t \bigg]$.</p>
						<p>Градиент $J(\theta)$:</p>
						<p class="equation">$\nabla_{\theta}J(\theta) = \mathbb{E}_{\tau \sim \pi{\theta}}\bigg[\sum^T\limits_{t=0}\nabla_{\theta}\ln{\pi_{\theta}(a_t \mid s_t)}\cdot A^{\pi}(s_t, a_t) \bigg]$.</p>
						<p>Оценка функции преимущества:</p>
						<p class="equation">$A^{\pi}(s_t, a_t) \approx r_t + \gamma V_{\phi}(s_{t+1}) - V_{\phi}(s_t)$.</p>
						<p>Обновление параметров политики (актора):</p>
						<p class="equation">$\theta \leftarrow \theta + \alpha \nabla J(\theta)$.</p>
						<p>Обновление параметров функции ценности (критика):</p>
						<p>Параметры функции ценности обновляются с использованием метода градиентного спуска для минимизации среднеквадратичной ошибки:</p>
						<p class="equation">$\phi \leftarrow \phi - \beta \nabla_{\phi}\bigg(r_t + \gamma V_{\phi}(s_{t+1}) - V_{\phi}(s_t)\bigg)^2$</p>
					</ol>
					<ul>
						<li>Актор — это "мозг", который решает, какое действие выполнить в текущей ситуации;</li>
						<li>Критик — это "оценщик", который говорит, насколько хороша текущая ситуация и насколько удачным было действие.</li>
					</ul>
				</div>
			</div>
		</div>
	</body>
	<script type="text/javascript" src="js/katex.js"></script>
	<script type="text/javascript" src="js/contrib/auto-render.js"></script>
	<script type="text/javascript" src="js/render.js"></script>
	<script type="text/javascript" src="js/display.js"></script>
	<script type="text/javascript" src="js/navigate.js"></script>
</html>