Строение RL агента. Стратегия, полезность, модель
Марковские процессы принятия решений и их связь с обучением с подкреплением. Примеры.
Уравнение Белмана. Уравнение оптимальности Беллмана.
Итерационные алгоритмы поиска оптимальной стратегии. Итерации по стратегии и итерации по полезностям.
Алгоритмы основанные на полезности. Алгоритм Монте Карло для поиска оптимальной стратегии. Достоинства и недостатки
Алгоритмы основанные на полезности. TD алгоритм. Достоинства и недостатки
Алгоритмы основанные на полезности. Q-learning. Достоинства и недостатки
Алгоритмы основанные на полезности. SARSA. Достоинства и недостатки
Аппроксимация функции полезности. Линейный подход. Целевая функция (функционал ошибки)
Аппроксимация функции полезности. Нелинейный подход. Deep Q Learning. Experience replay. Функционал ошибки
Аппроксимация функции полезности. Нелинейный подход. Double Q-learning. Dueling Network. Функционал ошибки
Алгоритмы основанные на стратегии. Crossentropy метод. Привести пример на основе клеточного мира. Функционал ошибки.
Алгоритмы основанные на стратегии. Policy gradient. Вывод алгоритма Reinforce.
Advantage actor-critic. Вывод.
Многорукий бандит. Постановка задачи. Exporation vs exploitation. Regret и методы его оценки
Многорукий бандит. Стратегии исследований. Теорема Lia and Robbins
Многорукий бандит. Верхняя доверительная граница (UCB1). Неравенство Хёфдинга
Многорукий бандит. Томпсон семплинг. Пример с рекламой (монеткой) с формулами пересчета
Формула Байеса. Подходы для расчета апостериорного распределения. Сопряженное априорное распределение. Примеры
Формула Байеса. Подходы для расчета апостериорного распределения на примере линейной регрессионной модели
Приближенные методы оценки апостериорных распределений. Projection sampling, importence sampling
Марковские цепи. Стационарность и эргодичность. Алгоритм Метрополиса-Гастингса оценки апостериорного распределения
Вариационные методы. EM алгоритм.
Вариационные методы. Приближенный вариационный вывод