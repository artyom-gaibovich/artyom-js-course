<!DOCTYPE html>
<html>
	<head>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1">
		<link rel="stylesheet" type="text/css" href="../../../css/page.css">
		<link rel="stylesheet" type="text/css" href="../../../css/katex.css">
		<title></title>
	</head>
	<body>
		<div class="page">
			<div class="sidebar">
				<a id="nextPage">
					<img src="../../../css/icons/forward.svg" title="Далее">
				</a>
				<a id="prevPage">
					<img src="../../../css/icons/backward.svg" title="Назад">
				</a>
				<a href="index.html">
					<img src="../../../css/icons/submenu.svg" title="Подменю">
				</a>
				<a href="../../../index.html">
					<img src="../../../css/icons/menu.svg" title="Главное меню">
				</a>
				<a href="https://t.me/quessentry">
					<img src="../../../css/icons/tg.svg" class="icon" title="Связь со мной">
				</a>
			</div>
			<div class="elements">
				<div class="topic">
					<div class="num">
						<p>05</p>
					</div>
					<div class="title">
						<p>Алгоритмы: Монте-Карло, TD, Q-learning, SARSA.</p>
					</div>
				</div>
				<div class="content">
					<ol class="roman">
						<li>
							<p class="sign">Монте-Карло</p>
							<p>Метод Монте-Карло основан на обучении по эпизодам. Агент взаимодействует со средой до завершения эпизода, после чего обновляет value function на основе полной последовательности наград.</p>
							<p class="equation">$V(s) \leftarrow V(s) + \alpha(G_t - V(s))$,</p>
							<p>где $G_t$ — полная дисконтированная награда, $\alpha$ — скорость обучения.</p>
							<p>Достоинства:</p>
							<ul class="property">
								<li>не требует знания модели среды;</li>
								<li>прост в реализации;</li>
								<li>сходится к $V^*(s)$ при достаточном количестве эпизодов.</li>
							</ul>
							<p>Недостатки:</p>
							<ul class="property">
								<li>требует завершения эпизода для обновления $V(s)$;</li>
								<li>может быть неэффективным в средах с длинными эпизодами.</li>
							</ul>
						</li>
						<li>
							<p class="sign">Метод временных различий</p>
							<p>Метод TD объединяет идеи MC и динамического программирования. Он обновляет value function на основе частичных наблюдений, не дожидаясь завершения эпизода.</p>
							<p class="equation">$V(s_t) \leftarrow V(s_t) +\alpha(r_{t} + \gamma V(s_{t+1}) - V(s_t))$,</p>
							<p>где $r_{t}$ — награда, полученная после перехода из состояния $s$ в состояние $s'$.</p>
							<p>Достоинства:</p>
							<ul class="property">
								<li>не требует завершения эпизода;</li>
								<li>быстрее сходится по сравнению с Монте-Карло;</li>
								<li>подходит для online-обучения.</li>
							</ul>
							<p>Недостатки:</p>
							<ul class="property">
								<li>может быть медленным в больших пространствах состояний;</li>
								<li>может быть менее точным, чем MC, из-за использования бутстраппинга.</li>
							</ul>
						</li>
						<li>
							<p class="sign">Q-learning</p>
							<p>Это off-policy алгоритм, который обучает функцию полезности для пар состояние-действие $Q(s,a)$. Агент выбирает действия на основе текущей политики, но обновляет $Q$-значения на основе максимального $Q$-значения для следующего состояния.</p>
							<p class="equation">$Q(s,a) \leftarrow Q(s,a) +\alpha(r_{t+1} + \gamma \max_{a \in A}Q(s_{t+1}, a))$,</p>
							<p>где $r_t$ — награда, полученная после перехода из состояния $s$ в состояние $s'$.</p>
							<p>Достоинства:</p>
							<ul class="property">
								<li>off-policy: может обучаться независимо от текущей политики.;</li>
								<li>хорошо работает в стохастических средах.</li>
							</ul>
							<p>Недостатки:</p>
							<ul class="property">
								<li>может быть медленным в больших пространствах состояний.</li>
							</ul>
						</li>
						<li>
							<p class="sign">Модифицированный Q-learning: SARSA</p>
							<p>это on-policy алгоритм, который также обучает функцию $Q(s,a)$, но обновляет $Q$-значения на основе действия, выбранного текущей политикой.</p>
							<p class="equation">$Q(s,a) \leftarrow Q(s,a) +\alpha(r + \gamma Q(s', a') - Q(s,a))$.</p>
							<p>Достоинства:</p>
							<ul class="property">
								<li>on-policy: учитывает текущую политику;</li>
								<li>хорошо работает в стохастических средах.</li>
							</ul>
							<p>Недостатки:</p>
							<ul class="property">
								<li>может быть менее гибким, чем Q-Learning.</li>
							</ul>
						</li>
					</ol>
				</div>
			</div>
		</div>
	</body>
	<script type="text/javascript" src="../../../js/katex.js"></script>
	<script type="text/javascript" src="../../../js/contrib/auto-render.js"></script>
	<script type="text/javascript" src="../../../js/render.js"></script>
	<script type="text/javascript" src="../../../js/display.js"></script>
	<script type="text/javascript" src="../../../js/navigate.js"></script>
</html>