<!DOCTYPE html>
<html>
	<head>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1">
		<link rel="stylesheet" type="text/css" href="../../../css/page.css">
		<link rel="stylesheet" type="text/css" href="../../../css/katex.css">
		<title></title>
	</head>
	<body>
		<div class="page">
			<div class="sidebar">
				<a id="nextPage">
					<img src="../../../css/icons/forward.svg" title="Далее">
				</a>
				<a id="prevPage">
					<img src="../../../css/icons/backward.svg" title="Назад">
				</a>
				<a href="index.html">
					<img src="../../../css/icons/submenu.svg" title="Подменю">
				</a>
				<a href="../../../index.html">
					<img src="../../../css/icons/menu.svg" title="Главное меню">
				</a>
				<a href="https://t.me/quessentry">
					<img src="../../../css/icons/tg.svg" class="icon" title="Связь со мной">
				</a>
			</div>
			<div class="elements">
				<div class="topic">
					<div class="num">
						<p>04</p>
					</div>
					<div class="title">
						<p>Итерационные алгоритмы поиска оптимальной стратегии. Итерации по стратегии и итерации по полезностям.</p>
					</div>
				</div>
				<div class="content">
					<p class="sign">Итерации по стратегии</p>
					<p class="redline">Задача: оценить текущую стратегию $\pi$.</p>
					<p class="redline">На этом этапе для текущей стратегии $\pi$ вычисляется функция полезности $V^{\pi}(s)$, которая представляет ожидаемую награду при следовании стратегии $\pi$ из состояния $s$. Это делается путем решения уравнения Беллмана для заданной стратегии. Уравнение решается итеративно до сходимости:</p>
					<p class="equation">$V_{k+1}(s)=\sum_{a \in A}\pi(a \mid s) \sum_{s' \in S}P(s'\mid s, a)[R(s, a, s') + \gamma V_k(s')]$.</p>
					<p>После оценки $V_{\pi}(s)$ cтратегия улучшается путем выбора действий, которые максимизируют ожидаемую полезность:</p>
					<p class="equation">$\pi'(s) = \argmax_{a \in A}\sum_{s' \in S}P(s'\mid s, a)[R(s, a, s') + \gamma V^{\pi}(s')]$.</p>
					<p>Процесс повторяется до тех пор, пока стратегия $\pi$ не перестанет меняться.</p>
					<p class="sign">Итерации по полезности</p>
					<p>На каждой итерации функция полезности обновляется по правилу:</p>
					<p class="equation">$V_{k+1}(s) = \max_{a \in A}\sum_{s' \in S}P(s'\mid s, a)[R(s, a, s') + \gamma V_k(s')]$.</p>
					<p>В отличии от итерации по стратегиям, мы не получаем стратегию в явном виде. Промежуточные значения полезностей могут не соответствовать ни одной стратегии. После сходимости $V(s)$ к $V^*(s)$ оптимальная стратегия $\pi$ извлекается следующим образом:</p>
					<p class="equation">$\pi'(s) = \argmax_{a \in A}\sum_{s' \in S}P(s'\mid s, a)[R(s, a, s') + \gamma V^{\pi}(s')]$.</p>
				</div>
			</div>
		</div>
	</body>
	<script type="text/javascript" src="../../../js/katex.js"></script>
	<script type="text/javascript" src="../../../js/contrib/auto-render.js"></script>
	<script type="text/javascript" src="../../../js/render.js"></script>
	<script type="text/javascript" src="../../../js/display.js"></script>
	<script type="text/javascript" src="../../../js/navigate.js"></script>
</html>