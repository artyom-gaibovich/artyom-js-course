<!DOCTYPE html>
<html>
	<head>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1">
		<link rel="stylesheet" type="text/css" href="../../../css/page.css">
		<link rel="stylesheet" type="text/css" href="../../../css/katex.css">
		<title></title>
	</head>
	<body>
		<div class="page">
			<div class="sidebar">
				<a id="nextPage">
					<img src="../../../css/icons/forward.svg" title="Далее">
				</a>
				<a id="prevPage">
					<img src="../../../css/icons/backward.svg" title="Назад">
				</a>
				<a href="index.html">
					<img src="../../../css/icons/submenu.svg" title="Подменю">
				</a>
				<a href="../../../index.html">
					<img src="../../../css/icons/menu.svg" title="Главное меню">
				</a>
				<a href="https://t.me/quessentry">
					<img src="../../../css/icons/tg.svg" class="icon" title="Связь со мной">
				</a>
			</div>
			<div class="elements">
				<div class="topic">
					<div class="num">
						<p>03</p>
					</div>
					<div class="title">
						<p>Уравнение Беллмана. Уравнение оптимальности Беллмана.</p>
					</div>
				</div>
				<div class="content">
					<p class="redline">Функция полезности $V(s)$ марковского процесса возрождения — это ожидаемая отдача, которую получает агент, начиная с состояния $s$:</p>
					<p class="equation">$V(s)=\mathbb{E}[r(s,a) \mid s_t = s]$.</p>
					<p>Функцию полезности $V(s)$ можно представить в виде суммы двух слагаемых: немедленного вознаграждения $r_t$ и дисконтированного значения следующего состояния $γV(s_{t+1})$. Для заданной политики $\pi(a∣s)$, которая определяет вероятность выбора действия $a$ в состоянии $s$, уравнение Беллмана для функции ценности состояния $V^{\pi}(s)$ записывается следующим образом:</p>
					<p class="equation">$V^{\pi}(s)=\mathbb{E}_{a \sim \pi, s' \sim P}[r_t+\gamma V(s_{t+1}) \mid s_t=s] = \sum_a{\pi(a \mid s)}\sum_{s'}P(s' \mid s, a)[R(s,a,s')+\gamma V(s')]$</p>
					<p>Уравнение оптимальности Беллмана используется для нахождения оптимальной функции значения состояния $V^*(s)$, которая соответствует оптимальной политике $\pi^*$. Оно записывается как:</p>
					<p class="equation">$V^*(s) = max_{a \in A} \sum_{s' \in S} P(s' \mid s, a)[R(s, a, s') + \gamma V^*(s')]$</p>
					<p class="sign">Пример 1. Расчёт расчета уравнений Беллмана с детерминированной политикой</p>
					<p>Рассмотрим MDP с тремя состояниями $s_1$, $s_2$, $s_3$. В каждом состоянии два действия: $a_1$ и $a_2$. Вероятности переходов и награды заданы следующим образом:</p>
					<ol>
						<li>Состояние $s_1$
							<ul class="property">
								<li>Действие $a_1$:
									<ul>
										<li>переход в $s_2$ с вероятностью $0.7$ и наградой $R(s_1, a_1, s_2) = 1$;</li>
										<li>переход в $s_3$ с вероятностью $0.3$ и наградой $R(s_1, a_1, s_3) = 0$.</li>
									</ul>
								</li>
								<li>Действие $a_2$: переход в $s_1$ с вероятностью $1$ и наградой $R(s_1, a_2, s_1) = 0$.</li>
							</ul>
						</li>
						<li>Состояние $s_2$
							<ul class="property">
								<li>Действие $a_1$: переход в $s_3$ с вероятностью $1$ и наградой $R(s_2, a_1, s_3) = 2$.</li>
								<li>Действие $a_2$:
									<ul>
										<li>переход в $s_1$ с вероятностью $0.5$ и наградой $R(s_2, a_2, s_1) = 1$;</li>
										<li>переход в $s_2$ с вероятностью $0.5$ и наградой $R(s_2, a_2, s_2) = 0$.</li>
									</ul>
								</li>
							</ul>
						</li>
						<li>Состояние $s_3$
							<ul class="property">
								<li>Действие $a_1$: переход в $s_1$ с вероятностью $1$ и наградой $R(s_3, a_1, s_1) = 3$.</li>
								<li>Действие $a_2$: переход в $s_2$ с вероятностью $1$ и наградой $R(s_3, a_2, s_2) = 1$</li>
							</ul>
						</li>
					</ol>
					<p>Пусть политика $\pi$ детерминированная: в каждом состоянии выбирается действие $a_1$; $\gamma = 0.9$.</p>
					<p class="sign">Решение</p>
					<ol class="roman">
						<li>
							<p>В $s_1$ выбирается действие $a_1$. Тогда</p>
							<p class="equation">$V(s_1)=\bigg[\sum_{s'}P(s' \mid s, a)[R(s_1, a_1, s') + \gamma V(s')]\bigg]=$</p>
							<p class="equation">$=0.7[1+0.9V(s_2)] + 0.3[0+0.9V(s_3)] = 0.7 + 0.63V(s_2) + 0.27V(s_3)$.</p>
						</li>
						<li>
							<p>В $s_2$ выбирается действие $a_1$. Тогда</p>
							<p class="equation">$V(s_2)=\bigg[\sum_{s'}P(s' \mid s, a)[R(s_2, a_1, s') + \gamma V(s')]\bigg]=1\cdot(2+0.9V(s_3))=2+0.9V(s_3)$</p>
						</li>
						<li>
							<p>В $s_3$ выбирается действие $a_1$. Тогда</p>
							<p class="equation">$V(s_3)=\bigg[\sum_{s'}P(s' \mid s, a)[R(s_3, a_1, s') + \gamma V(s')]\bigg]=1\cdot(3+0.9V(s_1))=3+0.9V(s_3)$</p>
						</li>
					</ol>
					<p>Имеется система из трех уравнений:</p>
					<p class="equation">$\begin{cases} V(s_1) = 0.7 + 0.63V(s_2) + 0.27V(s_3), \\ V(s_2) = 2+0.9V(s_3), \\ V(s_3) = 3+0.9V(s_3). \end{cases} \Rightarrow \begin{cases} V(s_1) = 18.12, \\ V(s_2) = 19.38, \\ V(s_3) = 19.31. \end{cases}$</p>
					<p>Интерпретация результата: $V(s_i)$ — ожидаемая суммарная награда, начиная с состояния $s_i$, следуя политике $\pi$.</p>
					<p class="sign">Пример 2. Расчёт расчета уравнений Беллмана со стохастической политикой</p>
					<ul class="property">
						<li>Состояния $S = {s_1, s_2}$;</li>
						<li>Действия $A = {a_1, a_2}$;</li>
						<li>Функция перехода $P(s' \mid s, a)$:
							<ul>
								<li>Из $s_1$ при действии $a_1$: $P(s_1 \mid s_1,a_1)=0.7, \ P(s_2 \mid s_1, a_1) = 0.3$;</li>
								<li>Из $s_1$ при действии $a_2$: $P(s_1 \mid s_1,a_2)=0.4, \ P(s_2 \mid s_1, a_2) = 0.6$;</li>
								<li>Из $s_2$ при действии $a_1$: $P(s_1 \mid s_2,a_1)=0.5, \ P(s_2 \mid s_2, a_1) = 0.5$;</li>
								<li>Из $s_2$ при действии $a_2$: $P(s_1 \mid s_2,a_2)=0.9, \ P(s_2 \mid s_2, a_2) = 0.1$.</li>
							</ul>
						</li>
						<li>Функция награды $R(s, a, s')$:
							<ul>
								<li>$R(s_1, a_1, s_1) = 5, \ R(s_1, a_1, s_2) = -1$;</li>
								<li>$R(s_1, a_2, s_1) = 3, \ R(s_1, a_2, s_2) = 2$;</li>
								<li>$R(s_2, a_1, s_1) = 1, \ R(s_2, a_1, s_2) = 4$;</li>
								<li>$R(s_2, a_2, s_1) = 6, \ R(s_2, a_2, s_2) = 0$.</li>
							</ul>
						</li>
						<li>Стратегия $\pi$:
							<ul>
								<li>В состоянии $s_1$: $\pi(a_1 \mid s_1) = 0.6, \ \pi(a_2 \mid s_1) = 0.4$;</li>
								<li>В состоянии $s_2$: $\pi(a_1 \mid s_2) = 0.3, \ \pi(a_2 \mid s_2) = 0.7$.</li>
							</ul>
						</li>
					</ul>
					<p class="sign">Решение</p>
					<p>Расчет для состояния $s_1$:</p>
					<p class="equation">$V^{\pi}(s_1) = \pi(a_1 \mid s_1) \bigg[ P(s_1 \mid s_1, a_1) (R(s_1, a_1, s_1) + \gamma V^\pi(s_1)) + P(s_2 \mid s_1, a_1)(R(s_1, a_1, s_2) + \gamma V^\pi(s_2)) \bigg]
				 	+ \pi(a_2 \mid s_1) \bigg[ P(s_1 \mid s_1, a_2) (R(s_1, a_2, s_1) + \gamma V^\pi(s_1)) + P(s_2 \mid s_1, a_2) (R(s_1, a_2, s_2) + \gamma V^\pi(s_2)) \bigg] = $</p>
				 	<p class="equation">$= 0.6\bigg[0.7(5+0.9V^{\pi}(s_1)) + 0.3(-1+0.9V^{\pi}(s_2))\bigg]+0.4\bigg[0.4(3 + 0.9 V^\pi(s_1)) + 0.6(2 + 0.9V^\pi(s_2))\bigg]$</p>
				 	<p>Для состояния $s_2$ расчёт аналогичен.</p>
				 	<p>Имеем систему уравнений:</p>
				 	<p class="equation" style="font-size: 16px;">$\begin{cases} V^{\pi}(s_1) = 0.6\bigg[0.7(5+0.9V^{\pi}(s_1)) + 0.3(-1+0.9V^{\pi}(s_2))\bigg]+0.4\bigg[0.4(3 + 0.9 V^\pi(s_1)) + 0.6(2 + 0.9V^\pi(s_2))\bigg]; \\ \ \\ V^{\pi}(s_2) = 0.3\bigg[0.5(1+0.9V^{\pi}(s_1)) + 0.5(4+0.9V^{\pi}(s_2))\bigg]+0.7\bigg[0.9(6 + 0.9 V^\pi(s_1)) + 0.1(0 + 0.9V^\pi(s_2))\bigg]. \end{cases}$</p>
				 	<p>Уравнение оптимальности Беллмана:</p>
				 	<p class="equation">$V^*(s) = max_{a \in A} \sum_{s' \in S} P(s' \mid s, a)[R(s, a, s') + \gamma V^*(s')]$</p>
				 	<p>Имеем для состояний $s_1$ и $s_2$:</p>
				 	<p class="equation">$V^*(s_1) = \max_{a \in A}\bigg[0.7(5+0.9V^{\pi}(s_1)) + 0.3(-1+0.9V^{\pi}(s_2)); \ 0.4(3 + 0.9 V^\pi(s_1)) + 0.6(2 + 0.9V^\pi(s_2))\bigg]$</p>
				 	<p class="equation">$V^*(s_2) = \max_{a \in A}\bigg[0.5(1+0.9V^{\pi}(s_1)) + 0.5(4+0.9V^{\pi}(s_2)); \ 0.9(6 + 0.9 V^\pi(s_1)) + 0.1(0 + 0.9V^\pi(s_2))\bigg]$</p>
				</div>
			</div>
		</div>
	</body>
	<script type="text/javascript" src="../../../js/katex.js"></script>
	<script type="text/javascript" src="../../../js/contrib/auto-render.js"></script>
	<script type="text/javascript" src="../../../js/render.js"></script>
	<script type="text/javascript" src="../../../js/display.js"></script>
	<script type="text/javascript" src="../../../js/navigate.js"></script>
</html>