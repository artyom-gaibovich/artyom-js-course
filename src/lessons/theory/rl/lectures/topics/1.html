<!DOCTYPE html>
<html>
	<head>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1">
		<link rel="stylesheet" type="text/css" href="../../../css/page.css">
		<link rel="stylesheet" type="text/css" href="../../../css/katex.css">
		<title>Строение RL-агента</title>
	</head>
	<body>
		<div class="page">
			<div class="sidebar">
				<a id="nextPage">
					<img src="../../../css/icons/forward.svg" title="Далее">
				</a>
				<a id="prevPage">
					<img src="../../../css/icons/backward.svg" title="Назад">
				</a>
				<a href="index.html">
					<img src="../../../css/icons/submenu.svg" title="Подменю">
				</a>
				<a href="../../../index.html">
					<img src="../../../css/icons/menu.svg" title="Главное меню">
				</a>
				<a href="https://t.me/quessentry">
					<img src="../../../css/icons/tg.svg" class="icon" title="Связь со мной">
				</a>
			</div>
			<div class="elements">
				<div class="topic">
					<div class="num">
						<p>01</p>
					</div>
					<div class="title">
						<p>Строение RL агента. Стратегия, полезность, модель</p>
					</div>
				</div>
				<div class="content">
					<p class="redline">Обучение с подкреплением (RL) — набор методов, позволяющих агенту вырабатывать оптимальную стратегию при его взаимодействии со средой. В каждый момент времени $t$ агент может оказывать на среду некоторое воздействие $a_t$. В ответ он получает информацию — наблюдение $o_{t+1}$, характеризующее новое состояние среды $s_{t+1}$ (после совершения действия), и числовую характеристику успешности действия — награду $r_{t+1}$. На основе полученной к моменту времени $t$ информации (траектории) агент должен выбрать очередное действие $a_t$ так, чтобы максимизировать накопленное вознаграждение.</p> 
					<p>Агент RL может включать один или несколько компонентов.</p>
					<ol class="roman">
						<li>
							<p class="sign">Стратегия/политика (Policy)</p>
							<p>Стратегия $\pi$ определяет поведение агента. Алгоритм, по которому агент выбирает действие. Это функция, которая ставит в соответствие состоянию $s \in \mathcal{S}$ распределение вероятностей над действиями $a \in \mathcal{A}:$</p>
							<p class="equation">$\pi(a \mid s) = \mathbb{P}(a_t = a  \mid  s_t = s)$</p>
							<p>где</p>
							<ul>
								<li>$\mathcal{S}$ — множество состояний среды,</li>
								<li>$\mathcal{A}$ — множество действий агента,</li>
								<li>$a_t$ и $s_t$ — действие и состояние в момент времени $t$.</li>
							</ul>
							<p>Может быть детерминированной, т.е $\pi(s) = a$, или стохастической, где $\pi(a \mid s)$ задает вероятность выбора действия $a$ в состоянии $s$. Детерминированная стратегия $\pi(s)$ — это стратегия, в которой для каждого состояния $s$ агент всегда выбирает одно и то же действие $a$. В отличие от стохастической стратегии, где действия выбираются случайно в соответствии с распределением вероятностей, детерминированная стратегия однозначно определяет действие для каждого состояния.</p>
							<p class="sign">Пример детерминированной стратегии</p>
							<p>Рассмотрим среду GridWorld размером 3x3. В каждом состоянии агент может выбрать одно из четырех действий: Вверх, Вниз, Влево, Вправо. Детерминированная стратегия может быть представлена в виде таблицы или графически на сетке.</p>
							<table class="tablex" width="80%">
								<tr>
									<td>Состояние $s$</td><td>Действие $a$</td>
								</tr>
								<tr>
									<td>$(1,1)$</td><td>$\rightarrow$</td>
								</tr>
								<tr>
									<td>$(1,2)$</td><td>$\rightarrow$</td>
								</tr>
								<tr>
									<td>$(1,3)$</td><td>$\downarrow$</td>
								</tr>
								<tr>
									<td>$(2,1)$</td><td>$\uparrow$</td>
								</tr>
								<tr>
									<td>$...$</td><td>$...$</td>
								</tr>
							</table>
							<p>На сетке GridWorld детерминированную стратегию можно изобразить с помощью стрелок, показывающих выбранное действие для каждого состояния. Например:</p>
							<table class="tablex">
								<tr>
									<td>$(1,1)$ $\rightarrow$</td><td>$(1,2)$ $\rightarrow$</td><td>$(1,3)$ $\downarrow$</td>
								</tr>
								<tr>
									<td>$(2,1)$ $\uparrow$</td><td>$(2,2)$ $\rightarrow$</td><td>$(2,3)$ $\downarrow$</td>
								</tr>
								<tr>
									<td>$(3,1)$ $\uparrow$</td><td>$(3,2)$ $\leftarrow$</td><td>$(3,3)$ $\leftarrow$</td>
								</tr>
							</table>
							<p class="sign">Пример стохастической стратегии</p>
							<table class="tablex" width="80%">
								<tr>
									<td>Состояние $s$</td><td>Действие $a$</td><td>Вероятность $\pi(a \mid s)$</td>
								</tr>
								<tr>
									<td>$(1,1)$</td><td>$\rightarrow$</td><td>$0.7$</td>
								</tr>
								<tr>
									<td>$(1,1)$</td><td>$\downarrow$</td><td>$0.3$</td>
								</tr>
								<tr>
									<td>$(1,1)$</td><td>$\leftarrow$</td><td>$0.0$</td>
								</tr>
								<tr>
									<td>$(1,1)$</td><td>$\uparrow$</td><td>$0.0$</td>
								</tr>
								<tr>
									<td>$...$</td><td>$...$</td><td>$...$</td>
								</tr>
							</table>
						</li>
						<li>
							<p class="sign">Функция полезности (Value Function)</p>
							<p>Функция полезности оценивает долгосрочную выгоду от нахождения в состоянии $s$ или от выполнения действия $a$ в состоянии $s$ при следовании стратегии $\pi$.</p>
							<p class="sign">Функция полезности действия</p>
							<p class="equation">$V^{\pi}(s) = \mathbb{E}_{\pi}\bigg[\sum_{k=0}^{\infty}\gamma^kR_{t+k+1} \mid s_t = s)\bigg]$</p>
							<p>где</p>
							<ul>
								<li>$R_{t+k+1}$ — вознаграждение на шаге $t+k+1$,</li>
								<li>$\gamma$ — дисконт, который определяет важность будущих вознаграждений.</li>
							</ul>
							<p class="sign">Пример $V^{\pi}(s)$</p>
							<table class="tablex">
								<tr>
									<td>Состояние $s$</td><td>$V^{\pi}(s)$</td>
								</tr>
								<tr>
									<td>$(1,1)$</td><td>$0.5$</td>
								</tr>
								<tr>
									<td>$(1,2)$</td><td>$0.7$</td>
								</tr>
								<tr>
									<td>$(1,3)$</td><td>$1.0$</td>
								</tr>
								<tr>
									<td>$(2,1)$</td><td>$0.3$</td>
								</tr>
								<tr>
									<td>$...$</td><td>$...$</td>
								</tr>
							</table>
							<p class="sign">Функция полезности состояния ($Q$-функция)</p>
							<p class="equation">$Q^{\pi}(s, a) = \mathbb{E}_{\pi}\bigg[\sum_{k=0}^{\infty}\gamma^kR_{t+k+1} \mid s_t = s, a_t = a)\bigg]$</p>
							<p>$Q$-функция оценивает полезность выполнения действия $a$ в состоянии $s$ с последующим следованием стратегии $\pi$.</p>
							<p class="sign">Пример $Q^{\pi}(s, a)$</p>
							<table class="tablex" width="80%">
								<tr>
									<td>Состояние $s$</td><td>Действие $a$</td><td>$Q^{\pi}(s, a)$</td>
								</tr>
								<tr>
									<td>$(1,1)$</td><td>$\rightarrow$</td><td>$0.7$</td>
								</tr>
								<tr>
									<td>$(1,2)$</td><td>$\downarrow$</td><td>$0.5$</td>
								</tr>
								<tr>
									<td>$(1,3)$</td><td>$\leftarrow$</td><td>$0.9$</td>
								</tr>
								<tr>
									<td>$(2,1)$</td><td>$\uparrow$</td><td>$0.4$</td>
								</tr>
								<tr>
									<td>$...$</td><td>$...$</td><td>$...$</td>
								</tr>
							</table>
						</li>
						<li>
							<p class="sign">Модель среды (model)</p>
							<p>Модель предсказывает, что произойдет в среде в следующий момент времени, описывает динамику среды и может быть использована для планирования. Она состоит из двух компонентов:</p>
							<ul class="circle">
								<li>
									<p class="sign">Функция переходов</p>
									<p class="equation">$P(s' \mid s, a) = \mathbb{P}(s_{t+1}=s' \mid s_t = s, a_t = a)$</p>
									<p>определяет вероятность перехода в состояние $s'$ при выполнении действия $a$ в состоянии $s$.</p>
									<p class="sign">Пример $\mathbb{P}(s' \mid s, a)$</p>
									<p>Для состояния $s=(1,1)$ и $a = "\rightarrow"$</p>
									<table class="tablex">
										<tr>
											<td>$s'$</td><td>$\mathbb{P}(s' \mid s, a)$</td>
										</tr>
										<tr>
											<td>$(1,2)$</td><td>$0.8$</td>
										</tr>
										<tr>
											<td>$(1,1)$</td><td>$0.2$</td>
										</tr>
									</table>
								</li>
								<li>
									<p class='sign'>Функция вознаграждения</p>
									<p class="equation">$R(s, a, s') = \mathbb{E}[R_{t+1}  \mid  s_t = s, a_t = a, s_{t+1} = s']$</p>
									<p>Эта функция определяет ожидаемое вознаграждение за переход из состояния $s$ в состояние $s'$ при выполнении действия $a$.</p>
								</li>
								<p>Оптимальная стратегия $\pi^*$ может быть определена как набор действий, которые максимизируют накопленное вознаграждение:</p>
								<p class="equation">$\pi^* = \argmax_{\pi}\bigg[\sum^{\infty}_{t=0}\gamma^tR_{t+1} \mid \pi\bigg]$.</p>
							</ul>
						</li>
					</ol>
				</div>
			</div>
		</div>
	</body>
	<script type="text/javascript" src="../../../js/katex.js"></script>
	<script type="text/javascript" src="../../../js/contrib/auto-render.js"></script>
	<script type="text/javascript" src="../../../js/render.js"></script>
	<script type="text/javascript" src="../../../js/display.js"></script>
	<script type="text/javascript" src="../../../js/navigate.js"></script>
</html>