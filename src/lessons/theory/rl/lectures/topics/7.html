<!DOCTYPE html>
<html>
	<head>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1">
		<link rel="stylesheet" type="text/css" href="../../../css/page.css">
		<link rel="stylesheet" type="text/css" href="../../../css/katex.css">
		<title></title>
	</head>
	<body>
		<div class="page">
			<div class="sidebar">
				<a id="nextPage">
					<img src="../../../css/icons/forward.svg" title="Далее">
				</a>
				<a id="prevPage">
					<img src="../../../css/icons/backward.svg" title="Назад">
				</a>
				<a href="index.html">
					<img src="../../../css/icons/submenu.svg" title="Подменю">
				</a>
				<a href="../../../index.html">
					<img src="../../../css/icons/menu.svg" title="Главное меню">
				</a>
				<a href="https://t.me/quessentry">
					<img src="../../../css/icons/tg.svg" class="icon" title="Связь со мной">
				</a>
			</div>
			<div class="elements">
				<div class="topic">
					<div class="num">
						<p>07</p>
					</div>
					<div class="title">
						<p>Нелинейный подход. DQN. Experience Replay. Функционал ошибки.</p>
					</div>
				</div>
				<div class="content">
					<p class="redline">Цель аппроксимации остаётся той же — минимизировать ошибку:</p>
					<p class="equation">$L(\mathbf{w}) = \mathbb{E}\bigg[(V(s)-V(s,\mathbf{w}))^2\bigg], \ \ L(\mathbf{w}) = \mathbb{E}\bigg[(Q(s, a)-Q(s, a, \mathbf{w}))^2\bigg]$.</p>
					<p>DQN аппроксимирует функцию полезности $Q(s,a)$ с помощью нейронной сети, что позволяет справляться с задачами в пространствах состояний и действий высокой размерности и моделировать сложные нелинейные зависимости. Алгоритм DQN:</p>
					<ol class="roman">
						<li>Инициализация нейронной сети $\hat{Q}(s, a, \mathbf{w})$ со случайными параметрами $\mathbf{w}$.</li>
						<li>Для каждого шага
							<ul>
								<li>агент выбирает действие $a$ в состоянии $s$ (например, с использованием $\epsilon$-жадной стратегии;</li>
								<li>агент выполняет действие $a$, переходит в состояние $s'$ и получает награду $r$;</li>
								<li>опыт $(s,a,r,s')$ сохраняется в буфере воспроизведения $D$ (experience replay);</li>
								<li>выбирается мини-батч (случайная подвыборка) $(s,a,r,s')$ опытов из буфера памяти, и параметры $\mathbf{w}$ обновляются для минимизации ошибки; подобная техника позволяет нарушить временную корреляцию между последовательными состояниями, что стабилизирует обучение.</li>
							</ul>
						</li>
					</ol>
					<p>Функционал ошибки в DQN минимизирует разницу между текущим предсказанием сети и целевым значением, вычисленным по уравнению Беллмана:</p>
					<p class="equation">$L(\mathbf{w}) = \mathbb{E}\bigg[(Q_{\text{target}}-Q(s,a,\mathbf{w}))^2\bigg] = \mathbb{E}\bigg[(r + \gamma \max_{a'} \hat{Q}(s',a')-Q(s,a,\mathbf{w}))^2\bigg]$.</p>
				</div>
			</div>
		</div>
	</body>
	<script type="text/javascript" src="../../../js/katex.js"></script>
	<script type="text/javascript" src="../../../js/contrib/auto-render.js"></script>
	<script type="text/javascript" src="../../../js/render.js"></script>
	<script type="text/javascript" src="../../../js/display.js"></script>
	<script type="text/javascript" src="../../../js/navigate.js"></script>
</html>