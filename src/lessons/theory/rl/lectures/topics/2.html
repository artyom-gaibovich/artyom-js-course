<!DOCTYPE html>
<html>
	<head>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1">
		<link rel="stylesheet" type="text/css" href="../../../css/page.css">
		<link rel="stylesheet" type="text/css" href="../../../css/katex.css">
		<title></title>
	</head>
	<body>
		<div class="page">
			<div class="sidebar">
				<a id="nextPage">
					<img src="../../../css/icons/forward.svg" title="Далее">
				</a>
				<a id="prevPage">
					<img src="../../../css/icons/backward.svg" title="Назад">
				</a>
				<a href="index.html">
					<img src="../../../css/icons/submenu.svg" title="Подменю">
				</a>
				<a href="../../../index.html">
					<img src="../../../css/icons/menu.svg" title="Главное меню">
				</a>
				<a href="https://t.me/quessentry">
					<img src="../../../css/icons/tg.svg" class="icon" title="Связь со мной">
				</a>
			</div>
			<div class="elements">
				<div class="topic">
					<div class="num">
						<p>02</p>
					</div>
					<div class="title">
						<p>Марковские процессы принятия решений и их связь с RL. Примеры.</p>
					</div>
				</div>
				<div class="content">
					<p class="redline">Марковский процесс принятия решений (MDP) — это математическая модель для принятия решений в ситуациях, где outcomes частично случайны и частично находятся под контролем лица, принимающего решения. Формально, MDP определяется как кортеж $<\mathcal{S}, \mathcal{A}, P, R, \gamma>$, где:</p>
					<ul class="property">
						<li>$\mathcal{S}$ — множество состояний;</li>
						<li>$\mathcal{A}$ — множество действий;</li>
						<li>$P : \mathcal{S} \times \mathcal{A} \times \mathcal{S} = \{(s_t, a_t, s_{t+1})\}_{t=0}^{T} \rightarrow [0,1]$ — функция переходов, определяющая вероятность перехода из состояния $s$ в состояние $s'$ при выполнении действия $a$:
							<p class="equation">$P(s' \mid s, a) = \mathbb{P}(s_{t+1}=s' \mid s_t=s, a_t=a)$;</p>
						</li>
						<li>$R : \mathcal{S} \times \mathcal{A} \times \mathcal{S} \rightarrow \mathbb{R}$ — функция награды, определяющая ожидаемую награду за переход из состояния $s$ в состояние $s'$ при выполнении действия $a$:
							<p class="equation">$R(s, a, s') = \mathbb{E}(R_{t+1} \mid s_t=s, a_t=a, s_{t+1}=s')$</p>
						</li>
						<li>$\gamma \in [0,1]$.</li>
					</ul>
					<p>Среда может быть детерминированной или стохастической; стационарной или нестационарной. В среде могут быть терминальные состояния, при достижении которых эпизод заканчивается. Максимизация суммарного вознаграждения означает минимизацию времени достижения терминального состояния. В процессе обучения агент может формировать модель среды, которая предсказывает вероятность следующего наблюдения на основе траектории. Если текущее состояние среды $s_t$ содержит всю информацию, необходимую для принятия решения, то такая среда называется марковской. Это означает, что условная вероятность перехода в новое состояние и награда зависят только от текущего состояния и действия, а не от истории предыдущих состояний и действий:</p>
					<p class="equation">$\mathbb{P}(s', r_{t+1} \mid s_t=s, a_t=a, s_{t-1}, a_{t-1}, ... s_0, a_0)=\mathbb{P}(s', r_{t+1} \mid s_t=s, a_t=a)$</p>
					<p>Если известно текущее состояние, то история может быть отброшена, т.е. состояние является достаточной статистикой для будущего. Выбор оптимальной стратегии в такой среде называется марковским процессом принятия решения.</p>
					<p class="sign">Ключевые моменты связи</p>
					<ul class="property">
						<li>MDP формально описывает среду, в которой действует агент;</li>
						<li>MDP задает структуру задачи, а RL решает её через обучение на опыте;</li>
						<li>RL предоставляет методы для обучения оптимальной политики в рамках MDP;</li>
						<li>В RL агент может обучаться, даже если модель среды (функции $R$ и $P$) неизвестна, используя только взаимодействие со средой.</li>
					</ul>
					<p>Резюмируя: связь MDP и RL заключается в том, что MDP предоставляет формальную модель среды, а RL — методы для обучения агента в этой среде. MDP задает "правила игры", а RL — алгоритмы, которые позволяют агенту научиться играть в эту игру оптимально.</p>
					<p class="sign">Примеры MDP в RL</p>
					<ul>
						<li>Задача о блуждающем роботе
							<ul>
								<li>Состояния $S$: позиции робота на сетке;</li>
								<li>Действия $A$: движения вверх, вниз, влево, вправо;</li>
								<li>Награда $R$: $+1$ за достижение цели (терминальное состояние), $-1$ за попадание в препятствие, $0$ в остальных случаях;</li>
								<li>Функция переходов $P$: вероятность перехода в соседние клетки при выполнении действия.</li>
							</ul>
							В RL:
							<ol>
								<li>Агент начинает в случайном состоянии и выбирает действия (например, случайно или на основе текущей политики).</li>
								<li>После выполнения действия агент переходит в новое состояние и получает награду.</li>
								<li>Агент обновляет свою политику или функцию ценности на основе полученного опыта.</li>
								<li>Процесс повторяется до тех пор, пока агент не научится оптимальной политике.</li>
							</ol>
						</li>
						<li>Управление портфелем акций
							<ul>
								<li>Состояния $S$: текущее состояние портфеля и рыночные условия;</li>
								<li>Действия $A$: покупка, продажа или удержание акций;</li>
								<li>Награда $R$: прибыль или убыток от сделок;</li>
								<li>Цель: максимизировать долгосрочную прибыль.</li>
							</ul>
						</li>
					</ul>
				</div>
			</div>
		</div>
	</body>
	<script type="text/javascript" src="../../../js/katex.js"></script>
	<script type="text/javascript" src="../../../js/contrib/auto-render.js"></script>
	<script type="text/javascript" src="../../../js/render.js"></script>
	<script type="text/javascript" src="../../../js/display.js"></script>
	<script type="text/javascript" src="../../../js/navigate.js"></script>
</html>